{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["**Desarrollado por:** Javier Fernando Botía Valderrama\n","\n","*Docente del Departamento de Ingeniería de Sistemas*\n","\n","**Materia:** Análisis Avanzado de Datos\n","\n","**Departamento:** Ingeniería Aeroespacial\n","\n","**Facultad de Ingeniería - Universidad de Antioquia**"],"metadata":{"id":"3de1yQPk0dZC"}},{"cell_type":"markdown","source":["# Agrupamiento Difuso"],"metadata":{"id":"iZBbplUF0q-q"}},{"cell_type":"markdown","source":["El agrupamiento difuso es un modelo que permite asociar o relacionar cada dato o elemento de un conjunto de datos a los grupos o clusters que se desean agrupar. A diferencia del agrupamiento generado por el algoritmo K-Means, el agrupamiento difuso establece la relación de los datos con los clusters o grupos mediante una **matriz de grados de pertenencia, $\\mathbf{U}$**, el cual cada elemento de dicha matriz establece el grado de pertenencia de un elemento o dato a un cluster. La matriz $\\mathbf{U}$ tiene la siguiente configuración:\n","\n","$$\\mathbf{U} = \\begin{bmatrix} \\mu_{1,1} & \\cdots & \\mu_{1,c} & \\cdots &\\mu_{1,K} \\\\ \\vdots & \\ddots & \\vdots & \\cdots & \\vdots \\\\ \\mu_{n,1} & \\cdots & \\mu_{n,c} & \\cdots &\\mu_{n,K} \\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ \\mu_{N,1} & \\cdots & \\mu_{N,c} & \\cdots &\\mu_{N,K}\\end{bmatrix}_{N \\, x\\, K}$$\n","\n","donde $\\mu_{n,c}$ es el grado de pertenencia de una muestra o fila de la base de datos con respecto a una cluster $c$. La matriz $\\mathbf{U}$ tiene un tamaño de $N$ muestras o filas de la base de datos por $K$ clusters o grupos. Cada grado de pertenencia tiene un valor entre $0$ y $1$, lo cual si un grado de pertenencia es igual a $1$, correspondería al máximo valor del grado de pertenencia. \n","\n","Si se desea extraer el *vector de clases* para representar el agrupamiento de datos desde la matriz $\\mathbf{U}$, se calcula el *argumento del máximo valor* para cada muestra o fila de la base de datos:\n","\n","$$Indice_n = arg \\max_\\limits{n}\n","   \\lbrace \\mu_{n,1}, \\cdots, \\mu_{n,c}, \\cdots, \\mu_{n,K}\\rbrace$$\n","\n","donde $Indice_n$ es la etiqueta de la clase donde se encontro el máximo grado de pertenencia de la muestra o fila de la base de datos $n$.\n","\n","**Ejemplo:**\n","\n","$$\\mathbf{U} = \\begin{bmatrix} 0.3 & 0.3 & 0.4 \\\\ 0.1 & 0.5 & 0.4 \\\\ 0.9 & 0.04 & 0.01 \\\\ 0.7 & 0.2 & 0.1\\end{bmatrix}$$\n","\n","$$Indice_{n=1} = arg \\max_\\limits{n = 1}\n","   \\lbrace 0.3, 0.3, 0.4\\rbrace = 0.4 (c = 3) = 3$$\n","$$Indice_{n=2} = arg \\max_\\limits{n = 2}\n","   \\lbrace 0.1, 0.5, 0.4\\rbrace = 0.5 (c = 2) = 2$$\n","$$Indice_{n=3} = arg \\max_\\limits{n = 3}\n","   \\lbrace 0.9, 0.04, 0.01\\rbrace = 0.9 (c = 1) = 1$$\n","$$Indice_{n=4} = arg \\max_\\limits{n = 4}\n","   \\lbrace 0.7, 0.2, 0.1\\rbrace = 0.7 (c = 1) = 1$$\n","\n","$$Indice = \\lbrace 3, 2, 1, 1 \\rbrace$$\n","\n","Uno de los algoritmos de agrupamiento difuso más populares es el algoritmo **Fuzzy C-Means** (Artículo original del algoritmo FCM: http://web-ext.u-aizu.ac.jp/course/bmclass/documents/FCM%20-%20The%20Fuzzy%20c-Means%20Clustering%20Algorithm.pdf), el cual esta basado en la teoría de los conjuntos difusos y el clásico algoritmo K-Means. Veamos un ejemplo práctico del uso de este algoritmo."],"metadata":{"id":"S1xddac77Pes"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"mKAxZrH40UIp"},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import matplotlib\n","import requests\n","import io"]},{"cell_type":"markdown","source":["## Carga de datos"],"metadata":{"id":"PHNmWtadNui9"}},{"cell_type":"markdown","source":["Para esta parte practica, utilizaremos una base de datos en bruto de la calidad del aire"],"metadata":{"id":"o_LVwK-rNyzo"}},{"cell_type":"code","source":["url = \"https://raw.githubusercontent.com/javierfernandobotia/AnalisisAvanzadoDatos/main/AirQualityUCI.csv\"\n","download = requests.get(url).content\n","columnas = ['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13','F14','F15']\n","data = pd.read_csv(io.StringIO(download.decode('utf-8')),sep=';',decimal = ',', names = columnas)\n","# Seleccion[amos desde la característica tau1 hasta g4, para realizar la tarea de aprendizaje no supervisado\n","display(data.head())"],"metadata":{"id":"Q_OCnoFULnW5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = data.drop(columns=['F14','F15']) # Removemos las dos últimas columnas\n","display(data.head())"],"metadata":{"id":"iVe0_Aqpbexk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.info()"],"metadata":{"id":"B_jCxwPOTEC5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.impute import MissingIndicator\n","\n","Indicador = MissingIndicator(missing_values = np.nan) # Decimos que datos debe buscar como datos faltantes\n","Indicador.fit(data) # Aplicamos el método de búsqueda con nuestra base de datos\n","Datos_Indicador = Indicador.transform(data)\n","POS = np.where(Datos_Indicador == True) # Se busca aquellos datos que tiene un valor Booleanos igual a True\n","print(\"Porcentaje de Datos Faltantes (%): \", 100*(len(POS[0])/(Datos_Indicador.shape[0]*Datos_Indicador.shape[1])))"],"metadata":{"id":"z_1k9sN0cLwc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.impute import SimpleImputer # Método de imputación \n","\n","imp = SimpleImputer(missing_values = np.nan, strategy= 'mean')\n","imp.fit(data)\n","Datos_Imputacion_Media = imp.transform(data)\n","Datos_Imputacion_Media = pd.DataFrame(Datos_Imputacion_Media, \n","                                      columns = ['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13'])\n","Datos_Imputacion_Media.info()"],"metadata":{"id":"bwcGotj_c_kW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.preprocessing import MinMaxScaler\n","\n","MM = MinMaxScaler(feature_range=(0, 1))\n","data_norm = MM.fit_transform(Datos_Imputacion_Media)\n","data_norm = pd.DataFrame(data_norm, columns = ['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10','F11','F12','F13'])\n","display(data_norm)"],"metadata":{"id":"esIik92yWeIN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Algoritmo Fuzzy C-Means (FCM)"],"metadata":{"id":"t3IOKeI0iWKj"}},{"cell_type":"markdown","source":["El primer paso es crear una función que permita aplicar el algoritmo FCM. Para este apartado, vamos a explorar el pseudocódigo del algoritmo FCM y después su implementación en código."],"metadata":{"id":"5zKh3SDiiaZN"}},{"cell_type":"markdown","source":["**Entrada:** $\\lbrace Datos = X, Cluster = K, Defusificacion = m, Maxima\\, iteracion = maxiter, Error = e\\rbrace$ \n","\n","**Salida:** $\\lbrace Matriz\\, Pertenencia = U, Centros = V, Funcion = J, Error = e \\rbrace$\n","\n","1) Generar una matriz de **centros aleatorios**, $V$.\n","\n","$$ V_{inicial} = \\begin{bmatrix} v_{1,1} & \\cdots & v_{1,d} & \\cdots & v_{1,D} \\\\\n","\\vdots & \\ddots & \\vdots & \\cdots & \\vdots \\\\ \n","v_{c,1} & \\cdots & v_{c,d} & \\cdots & v_{c,D} \\\\\n","\\vdots & \\cdots & \\vdots & \\ddots & \\vdots \\\\\n","v_{K,1} & \\cdots & v_{k,d} & \\cdots & v_{k,D} \\end{bmatrix}_{K x D}$$\n","\n","2) Calcular la distancia euclidiana, $d$.\n","\n","$$d_{n,c} = \\vert\\vert x_n - v_c \\vert\\vert^2_{A = I} = (x_n - v_c)^T A \\cdot(x_n - v_c) = (x_n - v_c)^T\\cdot(x_n - v_c)$$\n","\n","siendo $x_n$ un vector de los datos $X$, $I$ la matriz identidad, **$n$** una muestra y **$c$** un cluster.\n","\n","3) Calcular los grados de pertenencia, $\\mu_{n,c}$, organizando los valores en una matriz de grados de pertenencia, $U$\n","\n","$$\\mu_{n,c} = \\left \\lbrack \\sum_{c=1}^{K = 1} \\left(\\frac{d_{n,j}}{d_{n,c}} \\right )^{\\frac{2}{m-1}} \\right \\rbrack ^{-1}$$\n","\n","Ejemplo:\n","\n","$$d = \\begin{bmatrix} 0.65124238 & 0.84657243 & 0.55286725 & 0.43498079 \\\\\n","0.71870343 & 0.76755062 & 0.37233378 & 0.67988686 \\end{bmatrix}$$\n","$Con\\,\\, m = 2$\n","\n","$$d^{- \\left (\\frac{2}{m-1} \\right)} = \\begin{bmatrix} 2.35784197 & 1.39531337 & 3.27158545 & 5.28518065 \\\\\n","       1.93597864 & 1.69740683 & 7.21331857 & 2.16334959 \\end{bmatrix}$$\n","\n","$$\\sum_{c=1}^{K = 2} d^{- \\left (\\frac{2}{m-1} \\right)} = \\begin{bmatrix} 4.29382061 & 3.09272019 & 10.48490403 & 7.44853025 \\end{bmatrix}$$\n","\n","$$\\frac{d^{- \\left (\\frac{2}{m-1} \\right)}}{\\sum_{c=1}^{K = 2} d^{- \\left (\\frac{2}{m-1} \\right)}} = \\frac{ \\begin{bmatrix} 2.35784197 & 1.39531337 & 3.27158545 & 5.28518065 \\\\\n","       1.93597864 & 1.69740683 & 7.21331857 & 2.16334959 \\end{bmatrix}} {\\begin{bmatrix} 4.29382061 & 3.09272019 & 10.48490403 & 7.44853025 \\\\\n","       4.29382061 & 3.09272019 & 10.48490403 & 7.44853025 \\end{bmatrix}}$$\n","       \n","$$U = \\begin{bmatrix} 0.54912447 & 0.45116056 & 0.31202817 & 0.70956021 \\\\\n","       0.45087553 & 0.54883944 & 0.68797183 & 0.29043979 \\end{bmatrix}$$\n","       \n","$$\\sum_{c=1}^{K = 2} \\mu_{n,c} = \\begin{bmatrix} 1 & 1 & 1 & 1\\end{bmatrix}$$\n","\n","4) Actualizar la matriz de centros, $V$, usando la siguiente ecuación:\n","\n","$$v_c = \\frac{\\sum_{n=1}^{N} \\mu_{n,c}^m \\cdot x_{n}}{\\sum_{n=1}^{N} \\mu_{n,c}^m} = \\frac{(\\mu^m)^T x}{\\sum_{n=1}^{N} \\mu_{n,c}^m}$$\n","\n","5) Calcular el valor de pérdida del agrupamiento, $J$, usando la siguiente ecuación:\n","\n","$$J = \\sum_{n=1}^{N} \\sum_{c=1}^{K} \\mu_{n,c}^{m} \\cdot d_{n,c}^2$$\n","\n","6) Calcular el error de agrupamiento, $\\vert\\vert U^{(i)} - U^{(i-1)}\\vert \\vert < e$. Si no se cumple, repetir los pasos 2), 3), 4), y 5) hasta alcanzar un error por debajo de $e$. \n","\n","Resultado Final del algoritmo FCM: Matriz de grados de pertenencia, $U$ y Matriz de centros actualizado, $V_{final}$.\n","\n","$$U = \\begin{bmatrix} \\mu_{1,1} & \\cdots & \\mu_{1,c} & \\cdots & \\mu_{1,K} \\\\\n","\\vdots & \\ddots & \\vdots & \\cdots & \\vdots \\\\\n","\\mu_{n,1} & \\cdots & \\mu_{n,c} & \\cdots & \\mu_{n,K} \\\\ \n","\\vdots & \\cdots & \\vdots & \\ddots & \\vdots \\\\\n","\\mu_{N,1} & \\cdots & \\mu_{N,c} & \\cdots & \\mu_{N,K}\\end{bmatrix}_{N x K}$$\n","\n","$$ V_{final} = \\begin{bmatrix} v_{1,1} & \\cdots & v_{1,d} & \\cdots & v_{1,D} \\\\\n","\\vdots & \\ddots & \\vdots & \\cdots & \\vdots \\\\ \n","v_{c,1} & \\cdots & v_{c,d} & \\cdots & v_{c,D} \\\\\n","\\vdots & \\cdots & \\vdots & \\ddots & \\vdots \\\\\n","v_{K,1} & \\cdots & v_{k,d} & \\cdots & v_{k,D} \\end{bmatrix}_{K x D}$$\n","\n","Una vez se finalice el algoritmo FCM, se procede a calcular el vector de clases para visualizar el agrupamiento de los datos, por medio del cálculo del **máximo grado de pertenencia en cada muestra** $n$:\n","\n","$$Clases_n = arg \\max \\lbrace \\mu_{n,1},\\ldots, \\mu_{n,c}, \\ldots, \\mu_{n,K} \\rbrace $$\n","\n","**Ejemplo del cálculo del máximo grado de pertenencia en cada muestra**:\n","\n","$$U = \\begin{bmatrix} 0.3 & 0.2 & 0.5 \\\\\n","0.4 & 0.3 & 0.3 \\\\\n","0.3 & 0.5 & 0.2 \\\\\n","0.1 & 0.7 & 0.2 \\\\ \\end{bmatrix}_{4 x 3}$$\n","\n","$Clases_{n = 1} = arg \\max \\lbrace 0.3(c=1), 0.2(c=2), 0.5(c=3) \\rbrace  = 0.5 (c=3) = 3$\n","$Clases_{n = 2} = arg \\max \\lbrace 0.4(c=1), 0.3(c=2), 0.3(c=3) \\rbrace  = 0.4 (c=1) = 1$\n","$Clases_{n = 3} = arg \\max \\lbrace 0.3(c=1), 0.5(c=2), 0.2(c=3) \\rbrace  = 0.5 (c=2) = 2$\n","$Clases_{n = 4} = arg \\max \\lbrace 0.1(c=1), 0.7(c=2), 0.2(c=3) \\rbrace  = 0.7 (c=2) = 2$\n","\n","Por consiguiente, el vector de clases es:\n","\n","$Clases = \\lbrace 3, 1, 2, 2\\rbrace$"],"metadata":{"id":"sZ8Sr09AUsvJ"}},{"cell_type":"markdown","source":["A partir del pseudocódigo, se crea una función llamada **FCM_Inicial**, para obtener una matriz de grados de pertenencia inicial, **u_0**, y el valor de pérdida de agrupamiento inicial **J**. Recuerden que la matriz de centros se genera aleatoriamente para iniciar el algoritmo."],"metadata":{"id":"FFkTc_rDT59m"}},{"cell_type":"code","source":["# X = datos, K = número de clusters, y m = parámetro de difusificación\n","def FCM_Inicial(X, K, m):\n","  import numpy as np\n","  from scipy.spatial import distance\n","  D = X.shape[1]\n","  centros = np.random.rand(K,D) # Generar una matriz de centros aleatorios.\n","  d = distance.cdist(X, centros) # Cálculo de la distancia Euclidiana entre los datos y los centros.\n","  d = d.T\n","  u_0 = d ** (- 2. / (m - 1)) # calculo de la distancia euclidiana elevado a la potencia (-2 / (m-1))\n","  u_0 /= np.ones((K, 1)).dot(np.atleast_2d(u_0.sum(axis=0))) # Matriz de grados de pertenencia inicial\n","  um = (u_0)**m\n","  J = (um*(d**2)).sum() # calculo del valor de pérdida del agrupamiento\n","  return u_0, J"],"metadata":{"id":"JwmvbAMEDavG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Luego, se crea una función llamada **FCM_Update**, para actualizar la matriz de centros y la matriz de grados de pertenencia, así como el cálculo de la pérdida de agrupamiento actualizado."],"metadata":{"id":"4_8WrmllT-8J"}},{"cell_type":"code","source":["def FCM_Update(X, K, m, u):\n","    import numpy as np\n","    from scipy.spatial import distance\n","    um = u**m\n","    centros = um.dot(X) / (np.ones((X.shape[1],1)).dot(np.atleast_2d(um.sum(axis=1))).T) # Actualizar centros\n","    d = distance.cdist(X, centros) # Cálculo de la distancia Euclidiana entre los datos y los centros\n","    d = d.T\n","    u_actual = d ** (- 2. / (m - 1))\n","    u_actual /= np.ones((K, 1)).dot(np.atleast_2d(u_actual.sum(axis=0))) # Actualizar matriz de grados de pertenencia U\n","    um = (u_actual)**m\n","    J = (um*(d**2)).sum()\n","   \n","    return u_actual, centros, J"],"metadata":{"id":"NKHcdn4uE-J8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Finalmente, se crea el programa principal llamado **FCM_main**, donde se integra **FCM_Inicial** y **FCM_Update**."],"metadata":{"id":"-ud08glAUE_A"}},{"cell_type":"code","source":["def FCM_main(X, K, m, error, max_iter):\n","    J = []\n","    err = []\n","    U_0, J_0 = FCM_Inicial(X, K, m) # Agrupamiento inicial\n","    J.append(J_0)\n","    i = 0\n","    for i in range(max_iter):\n","        if i == 0:\n","            u_actual, centros, J_1 = FCM_Update(X, K, m, U_0) # Actualización de la matriz U y la matriz V\n","            J.append(J_1) # Se almacena el valor de la pérdida del agrupamiento.\n","            u_1 = u_actual\n","            errores = np.linalg.norm(u_1 - U_0)  # Se calcula la norma entre U_0 o matriz de grados de pertenencia inicial\n","                                                 # y la matriz de grados de pertenencia actualizada u_1\n","            err.append(errores)\n","            if errores < error: # Se determina si se comple con la condición de finalización.\n","                break\n","        if i > 0:\n","            u_2, centros2, J_2 = FCM_Update(X, K, m, u_1) # Actualización de la matriz U y la matriz V\n","            J.append(J_2) # Se almacena el valor de la pérdida del agrupamiento.\n","            errores = np.linalg.norm(u_2 - u_1) # Se calcula la norma entre las matrices de grados de pertenencia u_2 y u_1\n","            err.append(errores)\n","            if errores < error: # Se determina si se comple con la condición de finalización.\n","                break\n","            u_1 = u_2\n","    return u_2.T, centros2, J, err # Se genera como resultado final la matriz de grados de pertenencia, la matriz de centros,\n","                                   # e"],"metadata":{"id":"PAIACtltFFgo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Una vez se construye el algoritmo FCM, se utiliza **FCM_main** para agrupar los datos, definiendo los siguientes parámetros para su uso:"],"metadata":{"id":"C4ua_-JgUKIB"}},{"cell_type":"code","source":["X = data_norm # Datos\n","K = 2 # Mumero de clusters\n","m = 2 # Indice de dedifusificación del algoritmo FCM\n","error = 1e-4 # Error mínimo del agrupamiento\n","max_iter = 1000 # Máximo Número de Iteraciones\n","\n","U, center, J, err = FCM_main(X, K, m, error, max_iter) # Algoritmo FCM"],"metadata":{"id":"fTF3ymbMFL0F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En la siguiente grafica, se puede apreciar el comportamiento de la pérdida del agrupamiento, $J$, con respecto al número de iteraciones del algoritmo FCM."],"metadata":{"id":"A0HGXzNnWPdt"}},{"cell_type":"code","source":["plt.plot(np.arange(len(J)), J, color=\"r\") # Generar gráfica con color rojo\n","plt.axhline(0, color=\"blue\") # Elegir color de la linea horizontal de referencia\n","plt.title('Perdida del agrupamiento - Cluster = {:.2f}'.format(K))\n","plt.xlabel('Iteraciones') # Etiqueta del eje x\n","plt.ylabel('Pérdida del Agrupamiento') # Etiqueta del eje y\n","plt.show() "],"metadata":{"id":"sOIyKe2qF9If"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["En esta grafica, se puede apreciar como el error del agrupamiento va decreciendo a medida que se incrementa el número de iteraciones. En algunos casos, se puede generar algunas oscilaciones debido a la pérdida del agrupamiento en cada iteración y la actualización de la matriz de centros del algoritmo FCM."],"metadata":{"id":"YZ_04dSpXEOH"}},{"cell_type":"code","source":["plt.plot(np.arange(len(err)), err, color=\"m\") # Generar gráfica con color rojo\n","plt.axhline(0, color=\"blue\") # Elegir color de la linea horizontal de referencia\n","plt.axhline(error, color=\"r\") # Elegir color de la linea horizontal de referencia del error dado por el usuario\n","plt.title('Error de Agrupamiento - Cluster = {:.2f}'.format(K))\n","plt.xlabel('Iteraciones') # Etiqueta del eje x\n","plt.ylabel('Error') # Etiqueta del eje y\n","plt.show()"],"metadata":{"id":"rPchUh0eGeOi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["A partir de la matriz de grados de pertenencia, $U$, se obtiene el vector de clases."],"metadata":{"id":"SaKfn_0GZo6o"}},{"cell_type":"code","source":["clases = np.argmax(U, axis=1) # Genera el vector de clases a partir del argumento del máximo valor de una matriz\n","print(\"Vector de clases = \",clases) "],"metadata":{"id":"7HQPv9E8HY5w"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Seleccionamos dos características de la base de datos para graficar el espacio de características que permita visualizar el agrupamiento de datos generado por el algoritmo FCM:"],"metadata":{"id":"RS9WNQBEZz50"}},{"cell_type":"code","source":["x_1 = data_norm['F11']\n","x_2 = data_norm['F12']\n","colors = [\"b\", \"orange\"]\n","fig = plt.figure(figsize = (5,5))\n","\n","for j in range(K):\n","    plt.plot(x_1[clases == j], x_2[clases == j], '.', color = colors[j])\n","\n","for pt in center:\n","    plt.plot(pt[0],pt[1],'rs')\n","\n","plt.xlabel(\"F11\")\n","plt.ylabel(\"F12\")\n","plt.title(\"Agrupamiento de los datos\")\n","plt.show()"],"metadata":{"id":"vb-SlSBAHYzV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Como se explico en el algoritmo K-Means, es necesario buscar el número óptimo de clusters. Sin embargo, como el agrupamiento difuso es diferente al algoritmo que genera el algoritmo K-Means debido a la matriz de grados de pertenencia, es necesario usar otros índices de validación interna para el algoritmo FCM.\n","\n"],"metadata":{"id":"t_K8_wE6e8QK"}},{"cell_type":"markdown","source":["## Indices de Validación Interna para Algoritmos de Agrupamiento Difuso"],"metadata":{"id":"yNshJTDA6e3B"}},{"cell_type":"markdown","source":["El **Coeficiente de Partición (PC)** es una métrica que mide el grado de difuso de una matriz de grados de pertenencias, $\\mathbf{U}$. El máximo valor de $PC$ permite obtener el número óptimo de clusters. \n"],"metadata":{"id":"xqC4Z4NLjkK-"}},{"cell_type":"code","source":["def PC(u):\n","  import numpy as np\n","  N = u.shape[0]\n","  pc = (1/N)*(np.square(u).sum())\n","  return pc"],"metadata":{"id":"wZDk83PpHYa7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El **Coeficiente de Entropía (PE)** es una métrica que mide el grado de difuso de una matriz de grados de pertenencias, $\\mathbf{U}$, usando la métrica de entropia de Shannon. El mínimo valor de $PE$ permite obtener el número óptimo de clusters."],"metadata":{"id":"2D9K8rktmgVT"}},{"cell_type":"code","source":["def PE(u):\n","  import numpy as np\n","  N = u.shape[0]\n","  pe = -(1/N)*(u*np.log(u)).sum()\n","  return pe"],"metadata":{"id":"5MuWBmZP0iKZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El **Coeficiente de Partición Modificado (MPC)** es una mejora de la métrica PC que considera la dependencia de la matriz de grados de pertenencia con respecto al número de clusters. El máximo valor de $MPC$ permite obtener el número óptimo de clusters."],"metadata":{"id":"VMozrnB5m5gU"}},{"cell_type":"code","source":["def MPC(u):\n","  import numpy as np\n","  N = u.shape[0]\n","  C = u.shape[1]\n","  pc = (1/N)*(np.square(u).sum())\n","  mpc = 1 - ((C/(C - 1))*(1 - pc))\n","  return mpc"],"metadata":{"id":"jaEgXUcA61Sp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El **Coeficiente de Entropía Modificado (MPE)** es una mejora de la métrica $PE$ que considera la dependencia de la matriz de grados de pertenencia con respecto al número de clusters y el número de muestras de la base de datos. El mínimo valor de $MPE$ permite obtener el número óptimo de clusters."],"metadata":{"id":"LzCecUmiqprk"}},{"cell_type":"code","source":["def MPE(u):\n","  import numpy as np\n","  N = u.shape[0]\n","  C = u.shape[1]\n","  pe = -(1/N)*(u*np.log(u)).sum()\n","  mpe = (N*pe)/(N-C)\n","  return mpe"],"metadata":{"id":"RPQ9BwIz7C13"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El índice **Xie-Beni (XB)** es una métrica que considera la matriz de grados de pertenencia y la matriz de datos, lo cual permite obtener un valor promedio entre la compactación y la separabilidad de los clusters. El mínimo valor de $XB$ permite obtener el número óptimo de clusters."],"metadata":{"id":"dYEXFJXwrGas"}},{"cell_type":"code","source":["def XB(u,x,m,center):\n","  from scipy.spatial import distance\n","  import numpy as np\n","  N = u.shape[0]\n","  C = u.shape[1]\n","  u_m = u**m\n","  dist_x_v = distance.cdist(x, center)\n","  dist_v_v = distance.cdist(center, center)\n","  dist_v_v[dist_v_v == 0] = np.inf # Cuando la distancia de un centro de una clase 1 con el mismo centro\n","                                   # es decir, d(v1,v1) = 0, entonces se iguala a un valor inf que evita\n","                                   # errores en el cálculo de la mínima distancia entre centros de diferente\n","                                   # cluster o grupo\n","  xb = (u_m*dist_x_v).sum()/(N*dist_v_v.min())\n","  return xb"],"metadata":{"id":"qrfvmQfD7CxY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El índice **Fukuyama - Sugeno (FS)** es una métrica que permite evaluar la compactación y la separabilidad entre clusters, relacionando una medida de la pérdida del agrupamiento y una medida de la separabilidad entre clusters con respecto a la matriz de grados de pertenencia. El valor mínimo de $FS$ permite obtener el número óptimo de clusters."],"metadata":{"id":"wjyKeZhBrpMu"}},{"cell_type":"code","source":["def FS(u,x,m,center):\n","  from scipy.spatial import distance\n","  import numpy as np\n","  N = u.shape[0]\n","  C = u.shape[1]\n","  u_m = u**m\n","  media_centros = center.mean(axis = 0)\n","  dist_x_v = distance.cdist(x, center)\n","  dist_square_v_vmedia = np.linalg.norm(center - media_centros, axis=1, keepdims=True)**2\n","  fs = (u_m*dist_x_v).sum() - (u_m.T*dist_square_v_vmedia).sum()\n","  return fs"],"metadata":{"id":"UzO24npb7Csp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El índice **simétrico entre clusters (SYM)** es una métrica que evalua la calidad del agrupamiento difuso a partir del calculo de los puntos simétricos de cada cluster y la máxima distancia entre los centros. El valor máximo de $SYM$ permite obtener el número óptimo de clusters."],"metadata":{"id":"T7ltJzzht1zK"}},{"cell_type":"code","source":["def SYM_INDEX(u,x,center):\n","  from scipy.spatial import distance\n","  import numpy as np\n","  C = u.shape[1]\n","  dist_x_v = distance.cdist(x, center)\n","  E_K = (u*dist_x_v).sum()\n","  dist_v_v = distance.cdist(center, center)\n","  dist_v_v[dist_v_v == 0] = 0\n","  D_K = dist_v_v.max()\n","  syms_index = (1/C)*(1/E_K)*D_K\n","  return syms_index"],"metadata":{"id":"DvpN8BkxDRGJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["El índice **Wu - Li (WLI)** es una métrica que considera una medida de compactación por medio de una ponderación de las distancias difusas de los centros con los datos y la cardinalidad difusa de cada cluster (recuerden que en la teoría de conjuntos, la cardinalidad mide la cantidad de elementos de un conjunto). La separabilidad de esta métrica se establece mediante una distancia entre los centros, estableciendo un valor promedio entre la mínima distancia y la distancia mediana de los clusters. Al establecer una relación entre compactación y la separabilidad, el mínimo valor de $WLI$ permite establecer el número óptimo de clusters. "],"metadata":{"id":"QpduP4grsfH9"}},{"cell_type":"code","source":["def WLI(u,x,center):\n","  import numpy as np\n","  from scipy.spatial import distance\n","  dist_x_v = distance.cdist(x, center)\n","  u_2 = u**2\n","  L = (u_2*dist_x_v**2).sum(axis = 0)/u.sum(axis = 0)\n","  WLn = L.sum()\n","  dist_v_v = distance.cdist(center, center)\n","  dvv = dist_v_v**2\n","  dvv[dvv == 0] = np.inf\n","  nuevo_dvv = np.delete(dvv,range(0,dvv.shape[0]**2,(dvv.shape[0]+1))).reshape(dvv.shape[0],(dvv.shape[1]-1))\n","  WLd = (dvv.min() + np.median(nuevo_dvv))/2\n","  wli = WLn/2*WLd\n","  return wli "],"metadata":{"id":"QlUBo7ujG-o5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Búsqueda del Número Óptimo de Clusters en el Modelo del Agrupamiento Difuso"],"metadata":{"id":"FdDtkh1ACtW_"}},{"cell_type":"code","source":["data_norm.shape"],"metadata":{"id":"rX9EeP15Ms8z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["display(data_norm.head(5))"],"metadata":{"id":"cbT318p_PCrB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["fig1,axes1 = plt.subplots(3,3,figsize = (8,8))\n","\n","x_1 = data_norm['F11']\n","x_2 = data_norm['F12']\n","feature_A = 10 # Ubicación equivalente a 'F11'. Recuerde que en un arreglo, la posición inicia desde 0, por esto se define que\n","               # 'F11' esta ubicado en la columna 10 de la variable data_norm. Esto será útil para ubicar los centros.\n","feature_B = 11 # Ubicación equivalente a 'F12'\n","\n","U = []\n","C = []\n","Perdida_Agrupamiento = []\n","Error_Agrupamiento = []\n","Vector_Clases = []\n","\n","Tabla = []\n","\n","colors = [\"b\", \"orange\", \"g\", \"indigo\", \"c\", \"m\", \"y\", \"k\", \"Brown\", \"ForestGreen\"]\n","\n","m = 2\n","error = 1e-4\n","max_iter = 2000\n","\n","for ncenters, ax in enumerate(axes1.reshape(-1), 2):\n","  u, centros, J, err = FCM_main(data_norm, ncenters, m = m, error = error, max_iter = max_iter)\n","  clases = np.argmax(u, axis=1) # Extraemos el vector de clases a partir de la matriz U\n","  Vector_Clases.append(clases)\n","  U.append(u)\n","  C.append(centros)\n","  Perdida_Agrupamiento.append(J)\n","  Error_Agrupamiento.append(err)\n","  pc = PC(u)\n","  pe = PE(u)\n","  mpc = MPC(u)\n","  mpe = MPE(u)\n","  xb = XB(u,data_norm,m,centros)\n","  fs = FS(u,data_norm,m,centros)\n","  syms_index = SYM_INDEX(u,data_norm,centros)\n","  wli = WLI(u,data_norm,centros)\n","  Metricas = [ncenters, pc, pe, mpc, mpe, xb, fs, syms_index, wli]\n","  Tabla.append(Metricas)\n","\n","  for j in range(ncenters):\n","    ax.plot(x_1[clases == j], x_2[clases == j], '.', color = colors[j])\n","  \n","  for pt in centros:\n","    ax.plot(pt[feature_A],pt[feature_B],'rs')\n","\n","  ax.set_title('Cluster = {0}'.format(ncenters))\n","  ax.axis('off')\n","\n","fig1.tight_layout()\n","plt.show()"],"metadata":{"id":"MtsIx5Rk6Rd-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for j in range(ncenters-1):\n","  plt.plot(np.arange(len(Perdida_Agrupamiento[j-2])), Perdida_Agrupamiento[j-2], color=\"r\") # Generar gráfica con color rojo\n","  plt.axhline(0, color=\"blue\") # Elegir color de la linea horizontal de referencia\n","  plt.title('Perdida del agrupamiento - Cluster = {:.2f}'.format(j+2))\n","  plt.xlabel('Iteraciones') # Etiqueta del eje x\n","  plt.ylabel('Pérdida del Agrupamiento') # Etiqueta del eje y\n","  plt.show()"],"metadata":{"id":"n6Ku1sDZ6Yz8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for j in range(ncenters-1):\n","  post = len(Error_Agrupamiento[j-2]) \n","  Er = pd.DataFrame(Error_Agrupamiento[j-2])\n","  print(\"Error del Agrupamiento = \",Er[post-1:post].values)\n","  plt.plot(np.arange(len(Error_Agrupamiento[j-2])), Error_Agrupamiento[j-2], color=\"m\") # Generar gráfica con color rojo\n","  plt.axhline(0, color=\"blue\") # Elegir color de la linea horizontal de referencia\n","  plt.title('Error del agrupamiento - Cluster = {:.2f}'.format(j+2))\n","  plt.xlabel('Iteraciones') # Etiqueta del eje x\n","  plt.ylabel('Error del Agrupamiento') # Etiqueta del eje y\n","  plt.show()"],"metadata":{"id":"BDjPPYe7P7CY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["COL = ['Clusters', 'PC', 'PE', 'MPC', 'MPE', 'XB', 'FS', 'SYMS', 'WLI']\n","Tabla = pd.DataFrame(Tabla, columns = COL)\n","display(Tabla)"],"metadata":{"id":"spCpt26GrE2-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Valores_Optimos = [Tabla['PC'].max(), Tabla['PE'].min(), Tabla['MPC'].max(), Tabla['MPE'].min(), Tabla['XB'].min(), Tabla['FS'].min(),\n","                   Tabla['SYMS'].max(), Tabla['WLI'].min()]\n","pos_Optimos = [np.argmax(Tabla['PC']), np.argmin(Tabla['PE']), np.argmax(Tabla['MPC']), np.argmin(Tabla['MPE']),\n","               np.argmin(Tabla['XB']), np.argmin(Tabla['FS']), np.argmax(Tabla['SYMS']), np.argmin(Tabla['WLI'])]\n","Clust = Tabla['Clusters']\n","Clusters_Optimos = Clust[pos_Optimos]\n","Clusters_Optimos = Clusters_Optimos.reset_index()\n","Clusters_Optimos = Clusters_Optimos['Clusters'].values\n","Valores_Optimos = np.array(Valores_Optimos)\n","Tabla_Resumen = pd.DataFrame(np.vstack((Valores_Optimos, Clusters_Optimos)),columns = ['PC', 'PE','MPC', 'MPE', 'XB', 'FS','SYMS','WLI'], \n","                             index = ['Valor Métrica','Clusters óptimos'])\n","display(Tabla_Resumen)"],"metadata":{"id":"bjoFxm07w6sN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Etiquetas_Seleccionadas = Vector_Clases[0] # 0 es la posición donde esta ubicado el número óptimo de clusters"],"metadata":{"id":"nFPVcKdTUrYu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["Datos_Finales = pd.concat([Datos_Imputacion_Media,pd.DataFrame(Etiquetas_Seleccionadas,columns = ['Clase'])], axis = 1)\n","display(Datos_Finales)"],"metadata":{"id":"-opiQcyJflMc"},"execution_count":null,"outputs":[]}]}